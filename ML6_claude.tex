\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{listings}

\title{CS 5805 - Machine Learning I\\Homework \#6\\Simple \& Multiple Linear Regression Analysis}
\author{Student Name}
\date{October 30, 2025}

\begin{document}

\maketitle

\section{EDA (15 points)}

\subsection{ShelveLoc vs Sales Analysis}

\textbf{Question:} Plot the group horizontal bar plot that shows the 'ShelveLoc' versus Sales with respect to the 'US' (legend). Which Shelve location has the highest sales and whether the best sales are inside the US or outside?

\textcolor{red}{\textbf{NOTE: This plot is missing from the output file. The answer to which shelf location has the highest sales and whether it's in the US or outside is not provided in the output.}}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{}
    \caption{ShelveLoc vs Sales grouped by US location}
    \label{fig:shelveloc_sales}
\end{figure}

\subsection{One-Hot Encoding}

\textbf{Question:} Using one-hot encoding, convert qualitative features to quantitative features and make it ready for regression analysis. Avoid the dummy trap. Display the first 5 rows of the converted features.

\textbf{Answer:} The dataset was successfully encoded using one-hot encoding. The qualitative features (ShelveLoc, Urban, US) were converted to binary features while avoiding the dummy trap by dropping one category from each.

\begin{verbatim}
Encoded Head:
    Sales  CompPrice  Income  Advertising  Population  Price  Age  Education
0   9.50        138      73           11         276    120   42         17
1  11.22        111      48           16         260     83   65         10
2  10.06        113      35           10         269     80   59         12
3   7.40        117     100            4         466     97   55         14
4   4.15        141      64            3         340    128   38         13

   ShelveLoc_Good  ShelveLoc_Medium  Urban_Yes  US_Yes
0           False             False       True    True
1            True             False       True    True
2           False              True       True    True
3           False              True       True    True
4           False             False       True   False
\end{verbatim}

\subsection{Train-Test Split and Standardization}

\textbf{Question:} Split the dataset into train-test 80-20 then standardize the dataset. Turn shuffle ON and use random\_state = 5805. Display the first 5 rows of train and test set.

\textbf{Answer:} The dataset was split with 80\% training (320 samples) and 20\% testing (80 samples) with shuffle=True and random\_state=5805. The continuous features were standardized while encoded features were kept binary.

\textbf{Train Set (First 5 rows):}
\begin{verbatim}
         Sales  CompPrice    Income  Advertising  Population     Price
15   0.423881   1.528018  0.939806    -0.289507    0.957512  1.170049
186  0.413266  -0.355117 -0.614395    -1.055145   -1.161092 -1.296850
228 -0.747306   1.528018  0.162705     0.935514    0.826394  1.978171
182 -0.980836   0.748790 -0.296490    -0.442635   -0.215656  0.999918
236  0.639719   1.008533 -1.214882     1.394897    0.688374 -0.361130

          Age  Education  ShelveLoc_Good  ShelveLoc_Medium  Urban_Yes  US_Yes
15   1.411990   1.584919           False              True      False   False
186 -0.456133   1.196577           False              True      False   False
228 -1.701549  -1.133472           False             False      False    True
182 -1.763820  -0.356789           False             False       True   False
236  0.976095  -1.521813           False              True       True    True
\end{verbatim}

\textbf{Test Set (First 5 rows):}
\begin{verbatim}
         Sales  CompPrice    Income  Advertising  Population     Price
18   2.263812  -1.004474  1.469648    -1.055145    1.012720 -2.062439
372  0.101893  -0.290181 -0.649718    -1.055145    1.702819 -0.786457
9   -0.998528   0.424111  1.575616    -1.055145   -0.898854  0.319394
127 -0.351013  -0.030439 -0.720364    -0.595762   -0.477894 -0.020868
379 -0.602235  -0.030439  1.504971    -1.055145    0.985116 -0.403662

          Age  Education  ShelveLoc_Good  ShelveLoc_Medium  Urban_Yes  US_Yes
18  -0.456133   1.196577            True             False      False    True
372  0.727011  -1.133472           False              True      False   False
9    1.411990   1.196577           False              True      False    True
127 -0.144780   0.031553           False              True       True    True
379  0.042033   0.419894           False             False       True   False
\end{verbatim}

\textbf{Sales scaling parameters:}
\begin{itemize}
    \item Mean: 7.5120
    \item Scale (std): 2.8262
\end{itemize}

\section{Feature Selection \& Prediction: Backward Stepwise Regression (20 points)}

\subsection{Elimination Table}

\textbf{Question:} Create a table showing the backward stepwise regression process with one-by-one elimination monitoring AIC, BIC, and Adjusted R². Display p-values of eliminated features.

\textbf{Answer:} The backward stepwise regression started with 11 features and eliminated 4 features based on p-value threshold of 0.01.

\begin{table}[H]
\centering
\caption{Backward Stepwise Regression Elimination Process}
\begin{tabular}{cccccc}
\toprule
\textbf{Iteration} & \textbf{Features Count} & \textbf{Feature Eliminated} & \textbf{P-value} & \textbf{AIC} & \textbf{BIC} & \textbf{Adj. $R^2$} \\
\midrule
1 & 11 & Population & 0.962 & -633.132 & -587.913 & 0.867 \\
2 & 10 & Education & 0.599 & -635.130 & -593.679 & 0.867 \\
3 & 9 & US\_Yes & 0.338 & -636.843 & -599.160 & 0.867 \\
4 & 8 & Urban\_Yes & 0.213 & -637.893 & -603.978 & 0.867 \\
5 & 7 & None & - & -638.298 & -608.151 & 0.867 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Eliminated Features (4):}
\begin{enumerate}
    \item Population (p-value: 0.962)
    \item Education (p-value: 0.599)
    \item US\_Yes (p-value: 0.338)
    \item Urban\_Yes (p-value: 0.213)
\end{enumerate}

\textbf{Final Selected Features (7):}
\begin{enumerate}
    \item CompPrice
    \item Income
    \item Advertising
    \item Price
    \item Age
    \item ShelveLoc\_Good
    \item ShelveLoc\_Medium
\end{enumerate}

\subsection{OLS Summary Screenshots}

\textbf{Question:} Drop insignificant features and perform regression analysis OLS on selected features. Take screenshots of OLS summary after each elimination.

\textbf{Answer:} Five OLS regression summaries are provided showing the elimination process:

\textcolor{red}{\textbf{NOTE: Screenshots are not included in the output file. The text summaries are provided but not as image screenshots as requested.}}

\textbf{Final Model (Iteration 5) - 7 Features:}
\begin{itemize}
    \item R-squared: 0.871
    \item Adjusted R-squared: 0.868
    \item F-statistic: 299.8
    \item AIC: 269.8
    \item BIC: 300.0
    \item All features have p-value $\leq$ 0.01
\end{itemize}

\subsection{Final Regression Model}

\textbf{Question:} After removing insignificant features, write the final regression model.

\textbf{Answer:} The final regression equation with 7 significant features is:

\begin{equation}
\begin{aligned}
\text{Sales} = &-0.748 + 0.507 \times \text{CompPrice} + 0.168 \times \text{Income} \\
&+ 0.254 \times \text{Advertising} - 0.792 \times \text{Price} \\
&- 0.271 \times \text{Age} + 1.749 \times \text{ShelveLoc\_Good} \\
&+ 0.700 \times \text{ShelveLoc\_Medium}
\end{aligned}
\end{equation}

\textbf{Prediction vs Actual (First 10 test samples):}
\begin{verbatim}
   Actual_Sales Predicted_Sales Difference
0      2.263812        2.228757   0.035056
1      0.101893       -0.146335   0.248228
2     -0.998528       -0.471935  -0.526593
3     -0.351013       -0.279924  -0.071089
4     -0.602235       -0.469375  -0.132860
5     -0.800381       -1.127828   0.327447
6      0.289424        0.396014  -0.106589
7     -1.391282       -0.397780  -0.993502
8      1.050165        1.321531  -0.271366
9     -0.358090       -0.413283   0.055193
\end{verbatim}

\textcolor{red}{\textbf{NOTE: The plot of original test set versus predicted values is missing from the output.}}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{}
    \caption{Original Test Set vs Predicted Sales (Backward Stepwise)}
    \label{fig:stepwise_prediction}
\end{figure}

\subsection{Mean Squared Error}

\textbf{Question:} Display the MSE on this prediction.

\textbf{Answer:}
\begin{itemize}
    \item \textbf{Mean Squared Error (MSE): 0.984}
    \item Root Mean Squared Error (RMSE): 0.992
    \item Mean Absolute Error (MAE): 0.273
\end{itemize}

\section{Feature Selection \& Prediction: PCA (15 points)}

\subsection{Number of Features for 95\% Variance}

\textbf{Question:} How many features are needed that explain more than 95\% of the dependent variance?

\textbf{Answer:} \textbf{8 principal components} are needed to explain more than 95\% of the variance.

\begin{itemize}
    \item Total number of features: 11
    \item Number of components for 95\% variance: 8
    \item Cumulative variance explained: 0.9512 (95.12\%)
\end{itemize}

\begin{table}[H]
\centering
\caption{Principal Component Analysis - Variance Explained}
\begin{tabular}{ccc}
\toprule
\textbf{Component} & \textbf{Variance Explained} & \textbf{Cumulative Variance} \\
\midrule
PC1 & 0.208 & 0.208 \\
PC2 & 0.172 & 0.381 \\
PC3 & 0.134 & 0.515 \\
PC4 & 0.125 & 0.640 \\
PC5 & 0.122 & 0.762 \\
PC6 & 0.095 & 0.857 \\
PC7 & 0.053 & 0.910 \\
PC8 & 0.042 & \textbf{0.951} \\
PC9 & 0.026 & 0.977 \\
PC10 & 0.013 & 0.990 \\
PC11 & 0.010 & 1.000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Cumulative Explained Variance Plot}

\textbf{Question:} Plot the cumulative explained variance versus the number of features.

\textcolor{red}{\textbf{NOTE: This plot is missing from the output file.}}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{}
    \caption{Cumulative Explained Variance vs Number of Components}
    \label{fig:pca_variance}
\end{figure}

\subsection{95\% Threshold Lines}

\textbf{Question:} Draw vertical and horizontal lines showing the exact 95\% threshold and corresponding number of features.

\textcolor{red}{\textbf{NOTE: The plot with threshold lines is missing from the output file.}}

\subsection{PCA Interpretation}

\textbf{Question:} What does the PCA say? Does PCA tell you which feature(s) need to remove? Explain your answer.

\textbf{Answer:} \textcolor{red}{\textbf{NOTE: This explanation is missing from the output file. The student should explain that:}}
\begin{itemize}
    \item PCA transforms the original 11 features into 11 principal components
    \item 8 components capture 95.12\% of variance, suggesting some redundancy
    \item However, PCA does NOT directly tell which original features to remove
    \item PCA creates new composite features (principal components) that are linear combinations of original features
    \item To identify specific features to remove, other methods like backward elimination or random forest feature importance are more appropriate
\end{itemize}

\section{Feature Selection \& Prediction: Random Forest (25 points)}

\subsection{Feature Importance Bar Plot}

\textbf{Question:} Plot the horizontal bar graph displaying features importance in descending order.

\textbf{Answer:} The Random Forest analysis identified the following feature importances:

\begin{table}[H]
\centering
\caption{Random Forest Feature Importance}
\begin{tabular}{lc}
\toprule
\textbf{Feature} & \textbf{Importance} \\
\midrule
Price & 0.281 \\
ShelveLoc\_Good & 0.251 \\
CompPrice & 0.108 \\
Age & 0.106 \\
Advertising & 0.066 \\
Income & 0.057 \\
ShelveLoc\_Medium & 0.055 \\
Population & 0.037 \\
Education & 0.029 \\
Urban\_Yes & 0.005 \\
US\_Yes & 0.004 \\
\bottomrule
\end{tabular}
\end{table}

\textcolor{red}{\textbf{NOTE: The horizontal bar plot is missing from the output file.}}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{}
    \caption{Random Forest Feature Importance (Descending Order)}
    \label{fig:rf_importance}
\end{figure}

\subsection{Selected Features Comparison}

\textbf{Question:} Display eliminated and selected features. Are the selected features based on random forest and stepwise regression identical? Explain.

\textbf{Answer:} 

\textbf{Random Forest Selected Features (7) - Threshold: 0.05:}
\begin{enumerate}
    \item Price
    \item ShelveLoc\_Good
    \item CompPrice
    \item Age
    \item Advertising
    \item Income
    \item ShelveLoc\_Medium
\end{enumerate}

\textbf{Random Forest Eliminated Features (4):}
\begin{enumerate}
    \item Population
    \item Education
    \item Urban\_Yes
    \item US\_Yes
\end{enumerate}

\textbf{Comparison:} Yes, the selected features from Random Forest and Backward Stepwise Regression are \textbf{identical}. Both methods selected the same 7 features and eliminated the same 4 features (Population, Education, Urban\_Yes, US\_Yes).

\textcolor{red}{\textbf{NOTE: The student should provide more explanation about WHY they are identical - discussing that both methods identify features with weak predictive power, though through different mechanisms (statistical significance vs. tree-based importance).}}

\subsection{OLS Summary with Random Forest Features}

\textbf{Question:} Drop insignificant features and perform regression analysis OLS. Take screenshot of OLS summary.

\textcolor{red}{\textbf{NOTE: The OLS summary screenshot for Random Forest selected features is missing from the output file. However, since the features are identical to the stepwise regression, the OLS results should be the same as shown in Iteration 5 of Question 2.}}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{}
    \caption{OLS Summary with Random Forest Selected Features}
    \label{fig:rf_ols}
\end{figure}

\subsection{Prediction Plot}

\textbf{Question:} Make prediction using test set and plot original test set versus predicted values (not scatter plot).

\textcolor{red}{\textbf{NOTE: This plot is missing from the output file.}}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{}
    \caption{Original Test Set vs Predicted Sales (Random Forest Features)}
    \label{fig:rf_prediction}
\end{figure}

\subsection{Mean Squared Error}

\textbf{Question:} Display the MSE on this prediction.

\textbf{Answer:}
\begin{itemize}
    \item \textbf{Mean Squared Error (MSE): 0.9841}
    \item Root Mean Squared Error (RMSE): 0.9920
\end{itemize}

\section{Model Comparison (10 points)}

\textbf{Question:} Create a table comparing R-squared, Adjusted R-squared, AIC, BIC, and MSE of predictions in step 2 and 4. Which method of feature selection do you recommend and what features do you recommend for elimination? Explain.

\textbf{Answer:}

\begin{table}[H]
\centering
\caption{Model Comparison Summary}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{R²} & \textbf{Adj. R²} & \textbf{AIC} & \textbf{BIC} & \textbf{MSE} \\
\midrule
Backward Stepwise & 0.8706 & 0.8677 & 269.82 & 299.97 & 0.9841 \\
Random Forest & 0.8706 & 0.8677 & 269.82 & 299.97 & 0.9841 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Recommendation:} \textcolor{red}{\textbf{NOTE: Detailed recommendation is missing from output. The student should provide:}}

Both methods produced \textbf{identical results} in terms of all performance metrics:
\begin{itemize}
    \item Same R² and Adjusted R² (0.8706 and 0.8677)
    \item Same AIC and BIC (269.82 and 299.97)
    \item Same MSE (0.9841)
    \item Same 7 selected features
\end{itemize}

\textbf{Recommended Method:} Either method can be recommended, but:
\begin{itemize}
    \item \textbf{Backward Stepwise Regression} is preferred for interpretability as it provides statistical significance (p-values) for each feature
    \item \textbf{Random Forest} is preferred for capturing non-linear relationships and feature interactions
\end{itemize}

\textbf{Recommended Features for Elimination:}
\begin{enumerate}
    \item Population
    \item Education
    \item Urban\_Yes
    \item US\_Yes
\end{enumerate}

\section{Prediction Intervals (10 points)}

\textbf{Question:} Based on selected features from stepwise regression, find prediction intervals (95\%). Plot predicted sales value, lower and upper prediction intervals in one graph.

\textbf{Answer:}

\textbf{Prediction Summary (First 10 rows):}
\begin{table}[H]
\centering
\caption{Prediction Intervals (Standardized Scale)}
\scriptsize
\begin{tabular}{lcccccc}
\toprule
\textbf{Index} & \textbf{Mean} & \textbf{Mean SE} & \textbf{CI Lower} & \textbf{CI Upper} & \textbf{PI Lower} & \textbf{PI Upper} \\
\midrule
18 & 2.229 & 0.075 & 2.081 & 2.376 & 1.497 & 2.961 \\
372 & -0.146 & 0.042 & -0.228 & -0.065 & -0.868 & 0.575 \\
9 & -0.472 & 0.059 & -0.587 & -0.357 & -1.198 & 0.254 \\
127 & -0.280 & 0.033 & -0.345 & -0.215 & -1.000 & 0.440 \\
379 & -0.469 & 0.055 & -0.577 & -0.362 & -1.194 & 0.256 \\
362 & -1.128 & 0.059 & -1.244 & -1.012 & -1.854 & -0.402 \\
26 & 0.396 & 0.069 & 0.260 & 0.532 & -0.334 & 1.126 \\
356 & -0.398 & 0.079 & -0.552 & -0.243 & -1.131 & 0.336 \\
177 & 1.322 & 0.062 & 1.199 & 1.444 & 0.594 & 2.049 \\
131 & -0.413 & 0.047 & -0.506 & -0.321 & -1.136 & 0.310 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Prediction Intervals (Original Scale - First 10 samples):}
\begin{verbatim}
 Actual_Sales  Predicted_Sales  Lower_95%_PI  Upper_95%_PI  Within_Interval
        13.91         13.810926     11.742477     15.879375             True
         7.80          7.098459      5.059313      9.137605             True
         4.69          6.178252      4.126199      8.230305             True
         6.52          6.720913      4.686524      8.755301             True
         5.81          6.185488      4.136832      8.234144             True
         5.25          4.324572      2.272089      6.377055             True
         8.33          8.631242      6.569331     10.693152             True
         3.58          6.387829      4.315299      8.460358            False
        10.48         11.246932      9.191514     13.302351             True
         6.50          6.344013      4.301129      8.386898             True
\end{verbatim}

\textbf{Coverage Statistics:}
\begin{itemize}
    \item Coverage: 96.250\% of actual values fall within 95\% prediction intervals
    \item Expected: $\sim$95\%
    \item Average prediction interval width: 4.103
\end{itemize}

\textcolor{red}{\textbf{NOTE: The plot showing predicted sales with lower and upper prediction intervals is missing from the output file.}}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{}
    \caption{Predicted Sales with 95\% Prediction Intervals}
    \label{fig:prediction_intervals}
\end{figure}

\section{Polynomial Regression and Grid Search (25 points)}

\subsection{Grid Search for Polynomial Regression}

\textbf{Question:} Perform grid search that minimizes RMSE for polynomial regression of Sales vs Price.

\textbf{Answer:} Grid search was performed with 5-fold cross-validation for polynomial degrees 1 through 15.

\begin{itemize}
    \item Training samples: 320
    \item Testing samples: 80
    \item Cross-validation folds: 5
    \item Total fits: 75 (15 candidates × 5 folds)
\end{itemize}

\subsection{Optimum Polynomial Order}

\textbf{Question:} What is the optimum order for n?

\textbf{Answer:} \textbf{The optimum polynomial degree is n = 4} with a cross-validation RMSE of 2.565.

\subsection{RMSE vs Polynomial Order Plot}

\textbf{Question:} Plot RMSE versus n order for search interval [1, 15].

\textbf{Answer:}

\begin{table}[H]
\centering
\caption{RMSE for Each Polynomial Degree}
\begin{tabular}{cc}
\toprule
\textbf{Degree} & \textbf{RMSE} \\
\midrule
1 & 2.578 \\
2 & 2.586 \\
3 & 2.574 \\
\textbf{4} & \textbf{2.565} \\
5 & 2.570 \\
6 & 2.577 \\
7 & 2.586 \\
8 & 2.605 \\
9 & 2.646 \\
10 & 2.717 \\
11 & 2.816 \\
12 & 2.939 \\
13 & 3.075 \\
14 & 3.206 \\
15 & 3.312 \\
\bottomrule
\end{tabular}
\end{table}

\textcolor{red}{\textbf{NOTE: The plot of RMSE vs polynomial degree is missing from the output file.}}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{}
    \caption{RMSE vs Polynomial Degree (n)}
    \label{fig:poly_rmse}
\end{figure}

\subsection{Test Set Prediction Plot}

\textbf{Question:} Train regression model using OLS with optimum nth order. Plot test set sales versus predicted sales values.

\textcolor{red}{\textbf{NOTE: This plot is missing from the output file.}}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{}
    \caption{Test Set vs Predicted Sales (4th Order Polynomial)}
    \label{fig:poly_prediction}
\end{figure}

\subsection{Polynomial Regression MSE}

\textbf{Question:} What is the MSE of this nth order polynomial regression prediction?

\textbf{Answer:}
\begin{itemize}
    \item Polynomial degree: 4
    \item \textbf{Test Set MSE: 5.826}
    \item Test Set RMSE: 2.414
    \item Test Set R²: 0.255
\end{itemize}

\textcolor{red}{\textbf{NOTE: The polynomial regression MSE (5.826) is significantly worse than the multiple linear regression MSE (0.984), suggesting that using only the Price feature results in poor predictive performance compared to using all 7 significant features.}}

\section{Mathematical Proof (10 points)}

\textbf{Question:} In simple linear regression with n observations, prove the following formulas for $\hat{\beta}_0$ and $\hat{\beta}_1$ by minimizing the sum of squared errors:

\[
\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
\]

\[
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}
\]

\textbf{Answer:} \textcolor{red}{\textbf{NOTE: This mathematical proof is completely missing from the output file. The student needs to provide the derivation.}}

\textbf{Proof:}

For simple linear regression, we have the model:
\[
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

The sum of squared errors (SSE) is:
\[
\text{SSE} = \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^2
\]

To minimize SSE, we take partial derivatives with respect to $\beta_0$ and $\beta_1$ and set them to zero:

\textbf{Step 1: Partial derivative with respect to $\beta_0$}
\[
\frac{\partial \text{SSE}}{\partial \beta_0} = -2\sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i) = 0
\]

Simplifying:
\[
\sum_{i=1}^{n}y_i - n\beta_0 - \beta_1\sum_{i=1}^{n}x_i = 0
\]

\[
\beta_0 = \frac{1}{n}\sum_{i=1}^{n}y_i - \beta_1\frac{1}{n}\sum_{i=1}^{n}x_i = \bar{y} - \beta_1\bar{x}
\]

This gives us:
\[
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}
\]

\textbf{Step 2: Partial derivative with respect to $\beta_1$}
\[
\frac{\partial \text{SSE}}{\partial \beta_1} = -2\sum_{i=1}^{n}x_i(y_i - \beta_0 - \beta_1 x_i) = 0
\]

Simplifying:
\[
\sum_{i=1}^{n}x_i y_i - \beta_0\sum_{i=1}^{n}x_i - \beta_1\sum_{i=1}^{n}x_i^2 = 0
\]

Substituting $\beta_0 = \bar{y} - \beta_1\bar{x}$:
\[
\sum_{i=1}^{n}x_i y_i - (\bar{y} - \beta_1\bar{x})\sum_{i=1}^{n}x_i - \beta_1\sum_{i=1}^{n}x_i^2 = 0
\]

\[
\sum_{i=1}^{n}x_i y_i - \bar{y}\sum_{i=1}^{n}x_i + \beta_1\bar{x}\sum_{i=1}^{n}x_i - \beta_1\sum_{i=1}^{n}x_i^2 = 0
\]

Since $\sum_{i=1}^{n}x_i = n\bar{x}$:
\[
\sum_{i=1}^{n}x_i y_i - n\bar{x}\bar{y} + \beta_1n\bar{x}^2 - \beta_1\sum_{i=1}^{n}x_i^2 = 0
\]

Solving for $\beta_1$:
\[
\beta_1\left(\sum_{i=1}^{n}x_i^2 - n\bar{x}^2\right) = \sum_{i=1}^{n}x_i y_i - n\bar{x}\bar{y}
\]

\[
\hat{\beta}_1 = \frac{\sum_{i=1}^{n}x_i y_i - n\bar{x}\bar{y}}{\sum_{i=1}^{n}x_i^2 - n\bar{x}^2}
\]

\textbf{Step 3: Convert to the required form}

We can rewrite the numerator:
\[
\sum_{i=1}^{n}x_i y_i - n\bar{x}\bar{y} = \sum_{i=1}^{n}x_i y_i - \sum_{i=1}^{n}x_i\bar{y} - \sum_{i=1}^{n}\bar{x}y_i + n\bar{x}\bar{y}
\]
\[
= \sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})
\]

Similarly, the denominator:
\[
\sum_{i=1}^{n}x_i^2 - n\bar{x}^2 = \sum_{i=1}^{n}x_i^2 - 2\sum_{i=1}^{n}x_i\bar{x} + n\bar{x}^2 = \sum_{i=1}^{n}(x_i - \bar{x})^2
\]

Therefore:
\[
\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
\]

\textbf{Q.E.D.}

\section{Summary of Missing Elements}

\textcolor{red}{\textbf{The following items are missing from the output file:}}

\begin{enumerate}
    \item \textbf{Question 1a:} Horizontal bar plot of ShelveLoc vs Sales grouped by US, and the answer to which location has highest sales
    \item \textbf{Question 2c:} Plot of original test set vs predicted values (Backward Stepwise)
    \item \textbf{Question 3b:} Plot of cumulative explained variance vs number of components with 95\% threshold lines
    \item \textbf{Question 3d:} Detailed explanation of what PCA indicates about feature selection
    \item \textbf{Question 4a:} Horizontal bar plot of Random Forest feature importance
    \item \textbf{Question 4c:} OLS summary screenshot (though results should match Question 2 final model)
    \item \textbf{Question 4d:} Plot of original test set vs predicted values (Random Forest)
    \item \textbf{Question 5:} Detailed recommendation and explanation of which method to use
    \item \textbf{Question 6:} Plot of predicted sales with lower and upper 95\% prediction intervals
    \item \textbf{Question 7c:} Plot of RMSE vs polynomial degree
    \item \textbf{Question 7d:} Plot of test set vs predicted sales for 4th order polynomial
    \item \textbf{Question 8:} Complete mathematical proof for $\hat{\beta}_0$ and $\hat{\beta}_1$
\end{enumerate}

\section{Data Discrepancies}

\textcolor{red}{\textbf{Notable observations:}}

\begin{itemize}
    \item The polynomial regression (using only Price) has significantly worse performance (MSE = 5.826) compared to the multiple linear regression model (MSE = 0.984), which is expected since it uses only one predictor
    \item Both Backward Stepwise and Random Forest methods identified identical features, which provides strong validation of the feature selection
    \item The prediction interval coverage (96.25\%) is slightly higher than the expected 95\%, which is acceptable
    \item All numerical outputs appear to use 3-digit decimal precision as required
\end{itemize}

\end{document}